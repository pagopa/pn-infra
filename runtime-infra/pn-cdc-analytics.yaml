AWSTemplateFormatVersion: '2010-09-09'
Description: Create base resources for CDC files analysis and daily count
  reporting for Data Lake

Parameters:
  ProjectName:
    Type: String
    Default: pn
    Description: Base name for pn project
  LogsBucketName:
    Type: String
    Description: Logs bucket name
  BucketSuffix:
    Type: String
    Description: Suffix for the athena result bucket name
  LogsBucketKmsKeyArn:
    Type: String
    Description: Arn of logs bucket KMS key
  TemplateBucketBaseUrl:
    Type: String
    Description: The S3 bucket from which to fetch the templates used by this stack
  AlarmSNSTopicArn:
    Type: String
    Description: ARN of the SNS topic for alarms
  DataLakeCountQueryConfigParameterName:
    Type: String
    Description: SSM Parameter name for the JSON object containing custom count queries.
    Default: /datalake/count_queries/queries.json
  DataLakeCountTableList:
    Type: CommaDelimitedList
    Description: List of Athena table names to be processed for daily count.
    Default: pn_timelines,pn_userattributes,pn_mandate,pn_notifications,pn_radd_transaction_alt,pn_paper_notifications_cost,pn_asseveration

Resources:
  GlueServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - glue.amazonaws.com
            Action:
              - sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetBucketLocation
                  - s3:ListBucket
                  - s3:GetObject
                Resource:
                  - !Sub arn:aws:s3:::${LogsBucketName}
                  - !Sub arn:aws:s3:::${LogsBucketName}/*
        - PolicyName: S3EncryptionAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetEncryptionConfiguration
                Resource:
                  - !Sub arn:aws:s3:::${LogsBucketName}
        - PolicyName: KMSEncryptionAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - kms:Decrypt
                  - kms:DescribeKey
                Resource:
                  - !Ref LogsBucketKmsKeyArn

  AthenaWorkGroup:
    Type: AWS::Athena::WorkGroup
    DependsOn: GlueServiceRole
    Properties:
      Name: cdc_analytics_workgroup
      Description: Workgroup for querying data in Athena
      State: ENABLED
      WorkGroupConfiguration:
        EnforceWorkGroupConfiguration: false
        ResultConfiguration:
          OutputLocation: !Sub s3://${AthenaResultsBucket}/

  GlueDatabase:
    Type: AWS::Glue::Database
    DependsOn: AthenaWorkGroup
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: cdc_analytics_database

  AthenaResultsBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    UpdateReplacePolicy: Retain
    Properties:
      BucketName: !Sub ${ProjectName}-cdc-analytics-athena-results-${AWS::Region}-${AWS::AccountId}-${BucketSuffix}
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  AthenaResultsBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref AthenaResultsBucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: athena.amazonaws.com
            Action:
              - s3:PutObject
              - s3:GetObject
            Resource:
              - !Sub arn:aws:s3:::${AthenaResultsBucket}/*
            Condition:
              StringEquals:
                aws:SourceAccount: !Ref AWS::AccountId
          - Effect: Allow
            Principal:
              Service: athena.amazonaws.com
            Action:
              - s3:ListBucket
            Resource:
              - !Sub arn:aws:s3:::${AthenaResultsBucket}
            Condition:
              StringEquals:
                aws:SourceAccount: !Ref AWS::AccountId

  ###############################################################################
  ###                       UPDATE CDC JSON VIEWS CACHE                       ###
  ###############################################################################
  # Run with specific date: 
  # CLI example: aws lambda invoke --function-name PnAthenaUpdateCdcJsonCache --payload '{"date": "2025-02-15"}' output.txt
  # Console example: Use test event with payload {"date": "2025-02-15"}
  # Default: yesterday's date (UTC) if "date" is not specified
  UpdateCdcJsonViewsLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-AthenaUpdateCdcJsonCache'
      Description: Updates CDC JSON cache tables based on views
      Runtime: python3.11
      Handler: index.lambda_handler
      MemorySize: 256
      Timeout: 900
      Role: !GetAtt UpdateCdcJsonViewsLambdaRole.Arn
      Environment:
        Variables:
          REGION: !Ref AWS::Region
          ACCOUNT_ID: !Ref AWS::AccountId
          DATABASE_NAME: !Ref GlueDatabase
          ATHENA_OUTPUT_BUCKET: !Ref AthenaResultsBucket
          ATHENA_WORKGROUP: !Ref AthenaWorkGroup
      Code:
        ZipFile: |
          import boto3, json, time, os, logging, sys
          from datetime import datetime, timedelta
          from concurrent.futures import ThreadPoolExecutor
          from zoneinfo import ZoneInfo

          def setup_logger(aws_request_id):
              root = logging.getLogger()
              for h in root.handlers[:]:
                  root.removeHandler(h)
              
              fmt = "%(asctime)s %(aws_request_id)s %(levelname)s %(message)s"
              formatter = logging.Formatter(fmt=fmt, datefmt='%Y-%m-%dT%H:%M:%S')
              
              sh = logging.StreamHandler(sys.stdout)
              sh.setLevel(logging.INFO)
              sh.setFormatter(formatter)
              sh.addFilter(lambda record: setattr(record, 'aws_request_id', aws_request_id) or True)
              
              root.addHandler(sh)
              root.setLevel(logging.INFO)

          region = os.environ.get('REGION')
          account_id = os.environ.get('ACCOUNT_ID')
          database_name = os.environ.get('DATABASE_NAME')
          athena_output_bucket = os.environ.get('ATHENA_OUTPUT_BUCKET')
          athena_workgroup = os.environ.get('ATHENA_WORKGROUP')

          def execute_query(data, reference_date, start_time, timeout=880):
              """Execute and monitor Athena query with timeout handling"""
              athena = boto3.client('athena', region_name=region)
              
              # Build date condition and query
              date_filter = f"p_year = lpad(cast(year(date('{reference_date}')) as varchar), 4, '0') " + \
                          f"AND p_month = lpad(cast(month(date('{reference_date}')) as varchar), 2, '0') " + \
                          f"AND p_day = lpad(cast(day(date('{reference_date}')) as varchar), 2, '0')"
              
              query = f"INSERT INTO \"{database_name}\".\"{data['cache']}\" " + \
                      f"(SELECT * FROM \"{database_name}\".\"{data['view']}\" WHERE {date_filter} " + \
                      f"EXCEPT SELECT * FROM \"{database_name}\".\"{data['cache']}\" WHERE {date_filter})"
              
              # Start query and get execution ID
              resp = athena.start_query_execution(
                  QueryString=query,
                  WorkGroup=athena_workgroup,
                  ResultConfiguration={'OutputLocation': f"s3://{athena_output_bucket}/cache_updates/{data['cache']}"}
              )
              qid = resp['QueryExecutionId']
              result = {'table': data['cache'], 'id': qid}
              
              # Monitor until completion or timeout
              while True:
                  # Check for Lambda timeout
                  elapsed = time.time() - start_time
                  if elapsed > timeout:
                      logging.error(f"Query State TIMEOUT {qid} for {data['cache']}")
                      return {'table': data['cache'], 'id': qid, 'state': 'TIMEOUT', 'time': elapsed}
                      
                  # Check query status
                  status = athena.get_query_execution(QueryExecutionId=qid)['QueryExecution']['Status']['State']
                  if status == 'SUCCEEDED':
                      logging.info(f"Success: {data['cache']} in {time.time() - start_time:.1f}s")
                      return {'table': data['cache'], 'id': qid, 'state': 'SUCCESS', 'time': elapsed}
                  if status in ['FAILED', 'CANCELLED']:
                      reason = athena.get_query_execution(QueryExecutionId=qid)['QueryExecution']['Status'].get('StateChangeReason', 'Unknown')
                      logging.error(f"Query State {status} {data['cache']} - {reason}")
                      return {'table': data['cache'], 'id': qid, 'state': status, 'error': reason, 'time': elapsed}
                  time.sleep(2)

          def lambda_handler(event, context):
              try:
                  start_time = time.time()
                  aws_request_id = context.aws_request_id
                  setup_logger(aws_request_id)
                  
                  reference_date = event.get('date') or (datetime.now(ZoneInfo('UTC')) - timedelta(days=1)).strftime('%Y-%m-%d')
                  logging.info(f"Reference date: {reference_date}")
                  
                  glue = boto3.client('glue', region_name=region)
                  crawlers = glue.list_crawlers(Tags={'PnHasView': 'true'}, MaxResults=100).get('CrawlerNames', [])
                  
                  if not crawlers:
                      logging.info("No crawlers with PnHasView tag found")
                      return {'statusCode': 200, 'body': json.dumps({'message': 'No crawlers to process'})}
                  
                  queries = []
                  for crawler in crawlers:
                      tags = glue.get_tags(ResourceArn=f"arn:aws:glue:{region}:{account_id}:crawler/{crawler}")['Tags']
                      if 'PnView' in tags and 'PnViewCache' in tags:
                          queries.append({'view': tags['PnView'], 'cache': tags['PnViewCache']})
                  
                  results = []
                  with ThreadPoolExecutor(max_workers=20) as executor:
                      futures = {executor.submit(execute_query, q, reference_date, start_time): q['cache'] for q in queries}
                      for future in futures:
                          try:
                              results.append(future.result())
                          except Exception as e:
                              logging.error(f"Execution Status FAILED {futures[future]} - {str(e)}")
                              results.append({'table': futures[future], 'state': 'ERROR', 'error': str(e)})
                  
                  successes = [r for r in results if r.get('state') == 'SUCCESS']
                  failures = [r for r in results if r.get('state') == 'FAILED']
                  timeouts = [r for r in results if r.get('state') == 'TIMEOUT']
                  cancellations = [r for r in results if r.get('state') == 'CANCELLED']
                  
                  if failures or timeouts or cancellations:
                      logging.error(f"CDC JSON CACHE UPDATE ERRORS: {len(failures)} failures, {len(timeouts)} timeouts, {len(cancellations)} cancellations")
                  
                  total_time = time.time() - start_time
                  logging.info(f"Completed: {len(successes)} successes, {len(failures)} failures, {len(timeouts)} timeouts, {len(cancellations)} cancellations in {total_time:.1f}s")
                  
                  return {
                      'statusCode': 200,
                      'body': json.dumps({
                          'message': f"Completed in {total_time:.1f}s: {len(successes)} successes, {len(failures)} failures, {len(timeouts)} timeouts, {len(cancellations)} cancellations",
                          'execution_time': total_time,
                          'results': results
                      })
                  }
              except Exception as e:
                  logging.error(f"Unexpected error in lambda execution: {str(e)}")
                  raise
                      
  UpdateCdcJsonViewsLambdaAlarms:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: !Sub "${TemplateBucketBaseUrl}/fragments/lambda-alarms.yaml"
      Parameters:
        FunctionName: !Ref UpdateCdcJsonViewsLambda
        AlarmSNSTopicArn: !Ref AlarmSNSTopicArn

  UpdateCdcJsonViewsLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: pn-athena-update-cdc-json-cache-lambda-role
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: pn-athena-update-cdc-json-cache-lambda-policy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Sid: CanListCrawlers
                Effect: Allow
                Action:
                  - "glue:BatchGetCrawlers"
                  - "glue:ListCrawlers"
                  - "glue:GetTags"
                  - "glue:GetTable"
                  - "glue:GetTables"
                  - "glue:GetDatabase"
                  - "glue:GetDatabases"
                Resource: "*"
              - Sid: CanExecuteQuery
                Effect: Allow
                Action:
                  - "athena:BatchGetQueryExecution"
                  - "athena:GetQueryExecution"
                  - "athena:StartQueryExecution"
                Resource: "*"
              - Sid: CanCheckOutputBucketLocation
                Effect: Allow
                Action:
                  - s3:GetBucketLocation
                Resource:
                  - !Sub arn:aws:s3:::${AthenaResultsBucket}
              - Sid: WriteAndReadQueryResult
                Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                Resource:
                  - !Sub arn:aws:s3:::${AthenaResultsBucket}/cache_updates/*
              - Sid: ReadLogBucket
                Effect: Allow
                Action:
                  - s3:GetBucketLocation
                  - s3:GetEncryptionConfiguration
                  - s3:ListBucket
                  - s3:GetObject
                Resource:
                  - !Sub arn:aws:s3:::${LogsBucketName}
                  - !Sub arn:aws:s3:::${LogsBucketName}/*
              - Sid: DecriptLogBucket
                Effect: Allow
                Action:
                  - kms:Decrypt
                  - kms:DescribeKey
                Resource:
                  - !Ref LogsBucketKmsKeyArn
              - Sid: ReadWriteDestination
                Effect: Allow
                Action:
                  - s3:GetBucketLocation
                  - s3:GetEncryptionConfiguration
                  - s3:ListBucket
                  - s3:GetObject
                  - s3:PutObject
                Resource:
                  - !Sub arn:aws:s3:::${AthenaResultsBucket}
                  - !Sub arn:aws:s3:::${AthenaResultsBucket}/cdcTos3_parsed/*
              - Sid: EncryptDecriptDestination
                Effect: Allow
                Action:
                  - kms:Decrypt
                  - kms:Encrypt
                  - kms:GenerateDataKey
                  - kms:ReEncrypt*
                  - kms:DescribeKey
                Resource:
                  - !Ref LogsBucketKmsKeyArn

  UpdateCdcJsonViewsScheduleBridgeRule:
    Type: AWS::Scheduler::Schedule
    Properties: 
      Description: Schedule json views cache update everyday at 01:00 Europe/Rome
      ScheduleExpression: "cron(10 0 * * ? *)"
      FlexibleTimeWindow:
        Mode: "OFF"
      State: "ENABLED"
      Target:
        Arn: !GetAtt UpdateCdcJsonViewsLambda.Arn
        RoleArn: !GetAtt EventBusRunLambdaRole.Arn
  
  EventBusRunLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Action: sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - events.amazonaws.com
                - scheduler.amazonaws.com
      Policies:
        - PolicyName: runLambda
          PolicyDocument:
            Statement:
              - Sid: invokeLambda
                Action:
                  - "lambda:InvokeFunction"
                Effect: Allow
                Resource: 
                  - !GetAtt UpdateCdcJsonViewsLambda.Arn
                  - !GetAtt CdcDailyCountLambda.Arn

  AsseverationGlueTable:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref GlueDatabase
      TableInput:
        Name: pn_asseveration
        TableType: EXTERNAL_TABLE
        Parameters:
          EXTERNAL: 'true'
          projection.enabled: 'true'
          projection.p_year.type: integer
          projection.p_year.range: 2024,2099
          projection.p_month.type: integer
          projection.p_month.range: 01,12
          projection.p_month.digits: '2'
          projection.p_day.type: integer
          projection.p_day.range: 01,31
          projection.p_day.digits: '2'
          projection.p_hour.type: integer
          projection.p_hour.range: 00,23
          projection.p_hour.digits: '2'
          storage.location.template: !Join
            - ''
            - - s3://
              - !Ref LogsBucketName
              - /flussi/pad26_asseverazione/${p_year}/${p_month}/${p_day}/${p_hour}/
        StorageDescriptor:
          Location: !Sub s3://${LogsBucketName}/flussi/pad26_asseverazione/
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          SerdeInfo:
            SerializationLibrary: org.openx.data.jsonserde.JsonSerDe
          Columns:
            - Name: iun
              Type: string
            - Name: senderPaId
              Type: string
            - Name: notificationSentAt
              Type: string
            - Name: noticeCode
              Type: string
            - Name: creditorTaxId
              Type: string
            - Name: debtorPosUpdateDate
              Type: string
            - Name: recordCreationDate
              Type: string
            - Name: recipientIdx
              Type: int
            - Name: version
              Type: int
            - Name: moreFields
              Type: string
        PartitionKeys:
          - Name: p_year
            Type: string
          - Name: p_month
            Type: string
          - Name: p_day
            Type: string
          - Name: p_hour
            Type: string

  CdcDailyCountLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub pn-CdcDailyCountLambdaRole
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: CdcDailyCountLambdaPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Sid: SSMParameterAccess
                Effect: Allow
                Action: ssm:GetParameter
                Resource: !Sub arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter${DataLakeCountQueryConfigParameterName}
              - Sid: GlueCatalogAccess
                Effect: Allow
                Action:
                  - glue:GetTable
                  - glue:GetDatabase
                Resource:
                  - !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:catalog
                  - !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:database/${GlueDatabase}
                  - !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:table/${GlueDatabase}/*
              - Sid: AthenaQueryExecution
                Effect: Allow
                Action:
                  - athena:StartQueryExecution
                  - athena:GetQueryExecution
                  - athena:GetQueryResults
                Resource: !Sub arn:aws:athena:${AWS::Region}:${AWS::AccountId}:workgroup/${AthenaWorkGroup}
              - Sid: AthenaResultsBucketAccess
                Effect: Allow
                Action:
                  - s3:GetBucketLocation
                  - s3:GetObject
                  - s3:PutObject
                Resource:
                  - !Sub arn:aws:s3:::${AthenaResultsBucket}
                  - !Sub arn:aws:s3:::${AthenaResultsBucket}/*
              - Sid: LogsBucketReadAccess
                Effect: Allow
                Action:
                  - s3:GetBucketLocation
                  - s3:GetObject
                  - s3:ListBucket
                Resource:
                  - !Sub arn:aws:s3:::${LogsBucketName}
                  - !Sub arn:aws:s3:::${LogsBucketName}/*
              - Sid: LogsBucketCountWriteAccess
                Effect: Allow
                Action: s3:PutObject
                Resource: !Sub arn:aws:s3:::${LogsBucketName}/datalake_counts/*
              - Sid: KMSKeyAccess
                Effect: Allow
                Action:
                  - kms:Decrypt
                  - kms:Encrypt
                  - kms:GenerateDataKey
                Resource: !Ref LogsBucketKmsKeyArn

  CdcDailyCountLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "${ProjectName}-CdcDailyCountLambda"
      Description: "Performs daily counts on Athena tables for Data Lake reconciliation."
      Runtime: python3.12
      Handler: index.handler
      MemorySize: 256
      Timeout: 900
      Role: !GetAtt CdcDailyCountLambdaRole.Arn
      Environment:
        Variables:
          CONFIG_SSM_PARAMETER_NAME: !Ref DataLakeCountQueryConfigParameterName
          TABLE_LIST: !Join [ ",", !Ref DataLakeCountTableList ]
          OUTPUT_S3_BUCKET: !Ref LogsBucketName
          ATHENA_RESULTS_BUCKET: !Ref AthenaResultsBucket
          ATHENA_DATABASE: !Ref GlueDatabase
          ATHENA_WORKGROUP: !Ref AthenaWorkGroup
          MAX_WORKERS: "15"
      Code:
        ZipFile: |
          import boto3
          import json
          import os
          import time
          from datetime import datetime, timedelta, timezone
          from concurrent.futures import ThreadPoolExecutor
          from logging import getLogger, INFO

          # Module-level configuration
          logger = getLogger()
          logger.setLevel(INFO)
          
          ssm = boto3.client('ssm')
          glue = boto3.client('glue')
          athena = boto3.client('athena')
          s3 = boto3.client('s3')

          CONFIG_SSM_PARAM = os.environ['CONFIG_SSM_PARAMETER_NAME']
          TABLE_LIST = os.environ['TABLE_LIST']
          OUTPUT_BUCKET = os.environ['OUTPUT_S3_BUCKET']
          ATHENA_RESULTS_BUCKET = os.environ['ATHENA_RESULTS_BUCKET']
          DATABASE = os.environ['ATHENA_DATABASE']
          WORKGROUP = os.environ['ATHENA_WORKGROUP']
          MAX_WORKERS = int(os.environ['MAX_WORKERS'])

          # Validate required configuration at startup
          if not all([CONFIG_SSM_PARAM, TABLE_LIST, OUTPUT_BUCKET, ATHENA_RESULTS_BUCKET, DATABASE, WORKGROUP]):
              raise ValueError("Missing required environment variables")

          # --- Service Layer ---

          def fetch_custom_queries():
              """Fetch custom query configurations from SSM Parameter Store."""
              response = ssm.get_parameter(Name=CONFIG_SSM_PARAM, WithDecryption=True)
              return json.loads(response['Parameter']['Value'])

          def check_table_exists(table_name):
              """Check if table exists in Glue catalog."""
              try:
                  glue.get_table(DatabaseName=DATABASE, Name=table_name)
                  return True
              except glue.exceptions.EntityNotFoundException:
                  return False

          def execute_athena_count_query(query, output_prefix):
              """Execute Athena query and return count result."""
              response = athena.start_query_execution(
                  QueryString=query,
                  QueryExecutionContext={'Database': DATABASE},
                  ResultConfiguration={'OutputLocation': f"s3://{ATHENA_RESULTS_BUCKET}/{output_prefix}"},
                  WorkGroup=WORKGROUP
              )
              query_id = response['QueryExecutionId']
              
              # Wait for completion
              while True:
                  execution = athena.get_query_execution(QueryExecutionId=query_id)
                  status = execution['QueryExecution']['Status']['State']
                  if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
                      break
                  time.sleep(2)
              
              if status != 'SUCCEEDED':
                  reason = execution['QueryExecution']['Status'].get('StateChangeReason', 'Unknown')
                  raise RuntimeError(f"Query {query_id} failed: {reason}")

              # Get results
              results = athena.get_query_results(QueryExecutionId=query_id)
              rows = results['ResultSet']['Rows']
              if len(rows) > 1:
                  return int(rows[1]['Data'][0]['VarCharValue'])
              return 0

          def save_report_to_s3(report, date_path):
              """Save count report to S3."""
              key = f"datalake_counts/{date_path}/counts.json"
              s3.put_object(
                  Bucket=OUTPUT_BUCKET,
                  Key=key,
                  Body=json.dumps(report, indent=2)
              )
              logger.info(f"Report saved: s3://{OUTPUT_BUCKET}/{key}")

          # --- Business Logic ---

          def build_default_query(table_name, date_params):
              """Build default count query for a table."""
              # Default: count all CDC events (INSERT, MODIFY, REMOVE)
              # Alternative (commented): count events by type separately
              # SELECT 
              #   SUM(CASE WHEN eventName = 'INSERT' THEN 1 ELSE 0 END) as inserts,
              #   SUM(CASE WHEN eventName = 'MODIFY' THEN 1 ELSE 0 END) as modifies,
              #   SUM(CASE WHEN eventName = 'REMOVE' THEN 1 ELSE 0 END) as removes
              # FROM ...
              
              return (
                  f"SELECT COUNT(*) as total_count "
                  f"FROM \"{DATABASE}\".\"{table_name}\" "
                  f"WHERE p_year = '{date_params['YEAR']}' "
                  f"AND p_month = '{date_params['MONTH']}' "
                  f"AND p_day = '{date_params['DAY']}'"
              )

          def build_custom_query(template, table_name, date_params):
              """Build custom query from template."""
              return template.replace('{TABLE_NAME}', table_name).format(**date_params)

          def process_table(table_name, custom_configs, date_params):
              """Process a single table: check existence, build query, execute count."""
              if not check_table_exists(table_name):
                  logger.warning(f"Table '{table_name}' not found in Glue catalog - skipping")
                  return {
                      'table_name': table_name.replace('pn_', ''),
                      'send_count': None,
                      'status': 'NOT_FOUND'
                  }

              # Determine output name and query
              output_name = table_name.replace('pn_', '')
              
              if table_name in custom_configs:
                  config = custom_configs[table_name]
                  query = build_custom_query(config['query_template'], table_name, date_params)
                  output_name = config.get('output_table_name', output_name)
              else:
                  query = build_default_query(table_name, date_params)

              # Execute count
              count = execute_athena_count_query(query, f"athena_results/{table_name}")
              
              return {
                  'table_name': output_name,
                  'send_count': count
              }

          def process_all_tables(tables, custom_configs, date_params):
              """Process all tables in parallel."""
              results = []
              
              with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                  futures = {
                      executor.submit(process_table, table.strip(), custom_configs, date_params): table 
                      for table in tables
                  }
                  
                  for future in futures:
                      table = futures[future]
                      result = future.result()
                      results.append(result)
                      logger.info(f"Processed {table}: {result.get('send_count', 'N/A')}")
              
              return results

          def build_final_report(results, execution_timestamp):
              """Build final JSON report with execution timestamp."""
              return {
                  'tables': [
                      {**result, 'execution_timestamp': execution_timestamp}
                      for result in results
                  ]
              }

          # --- Entry Point ---

          def handler(event, context):
              """Lambda entry point - delegates to business logic."""
              # Calculate yesterday's date (T-1)
              yesterday = datetime.now(timezone.utc) - timedelta(days=1)
              date_params = {
                  'YEAR': yesterday.strftime('%Y'),
                  'MONTH': yesterday.strftime('%m'),
                  'DAY': yesterday.strftime('%d')
              }
              date_path = yesterday.strftime('%Y/%m/%d')
              execution_ts = datetime.now(timezone.utc).isoformat()
              
              logger.info(f"Processing counts for date: {yesterday.strftime('%Y-%m-%d')}")
              
              # Load configurations
              custom_configs = fetch_custom_queries()
              tables = [t.strip() for t in TABLE_LIST.split(',')]
              
              # Process all tables
              results = process_all_tables(tables, custom_configs, date_params)
              
              # Build and save report
              report = build_final_report(results, execution_ts)
              save_report_to_s3(report, date_path)
              
              return {'statusCode': 200, 'message': 'Count completed successfully'}

  # CdcDailyCountSchedule:
  #   Type: AWS::Events::Rule
  #   Properties:
  #     Name: !Sub "${ProjectName}-CdcDailyCountSchedule"
  #     Description: "Triggers the CdcDailyCountLambda daily at 01:00 UTC"
  #     ScheduleExpression: "cron(0 1 * * ? *)"
  #     State: "ENABLED"
  #     Targets:
  #       - Arn: !GetAtt CdcDailyCountLambda.Arn
  #         Id: "CdcDailyCountLambdaTarget"

  # CdcDailyCountLambdaPermission:
  #   Type: AWS::Lambda::Permission
  #   Properties:
  #     Action: "lambda:InvokeFunction"
  #     FunctionName: !GetAtt CdcDailyCountLambda.Arn
  #     Principal: "events.amazonaws.com"
  #     SourceArn: !GetAtt CdcDailyCountSchedule.Arn


Outputs:
  GlueDatabaseName:
    Description: Name of the Glue Database
    Value: !Ref GlueDatabase
    Export:
      Name: !Sub ${AWS::StackName}-GlueDatabaseName

  GlueDatabaseArn:
    Description: ARN of the Glue Database
    Value: !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:database/${GlueDatabase}
    Export:
      Name: !Sub ${AWS::StackName}-GlueDatabaseArn
  
  GlueServiceRoleArn:
    Description: ARN of the Glue Service Role
    Value: !GetAtt GlueServiceRole.Arn
    Export:
      Name: !Sub "${AWS::StackName}-GlueServiceRoleArn"

  AsseverationGlueTableName:
    Description: Name of the Glue Table for Asseveration
    Value: !Ref AsseverationGlueTable