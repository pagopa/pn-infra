AWSTemplateFormatVersion: 2010-09-09
Description: 'This template deploys the resources for the data monitoring'

Parameters:  
  ConfinfoDynamoDbExportBucketName:
    Type: String
    Default: ''
    Description: Name of the confinfo bucket created for the data monitoring (leave blank in Confinfo account)

  PnCoreAwsAccountId:
    Type: String
    Description: Core Account ID (leave blank in Core account)
    Default: ''

  ParquetBucketPath:
    Type: String
    Description: parquet files s3 bucket path
    Default: 'parquet'

  ReportsBucketPath:
    Type: String
    Description: reports files bucket path
    Default: 'reports'

  LogsBucketName:
    Type: String
    Description: Name of the logs bucket

  LogsBucketKmsKeyArn:
    Type: String
    Description: Name of the logs bucket KMS key ARN

  DataMonitoringIndexCronExpression:
    Type: String
    Description: Cron expression for the scheduled rule, e.g. cron(0 9 * * ? *)
    Default: 'cron(30 6 * * ? *)'

  AthenaResultsBucketName:
    Type: String
    Description: Name of the Athena results bucket

  #Dynamodb export parameters:

  DynamoDbExportTableNames:
    Type: String
    Default: ""
    Description: DynamoDB List of Table to export

  DynamoDbExportPrefix:
    Type: String
    Description: String Prefix after S3Prefix
    Default: incremental2024

  EnvironmentType:
    Type: String
    Description: Environment type (e.g. dev, test, prod)

  DataMonitoringScriptVersion:
    Type: String
    Description: Version of the bi_or_not_bi__ec_metadata scripts
    Default: ec_metadata_extract_v0_2

  DataMonitoringPrototypeVersion:
    Type: String
    Description: Version of the pn-prototype scripts
    Default: main

  DataLakeAccountId:
    Type: String
    Description: Datalake AWS Account ID
    Default: ''
  
  DataLakeRoleName:
    Type: String
    Description: Datalake Role Name
    Default: 'pagopa-pn'
  
  DataAnalysisSlackEmail:
    Type: String
    Description: Slack Email channel
    Default: ''

  LambdaPnSupportExportDynamoDbCronExpression:
    Type: String
    Description: Cron expression for the scheduled rule, e.g. cron(0 9 * * ? *)
    Default: ''

  LambdaPnSupportExportSqsCronExpression:
    Type: String
    Description: Cron expression for the scheduled rule, e.g. cron(0 9 * * ? *)
    Default: ''

  LambdaPnSupportExportDelete:
    Type: String
    Description: Condition for remove item form PaperRequestError DynamoDb table
    Default: "false"
    AllowedValues:
      - "true"
      - "false"

  LambdaPnSupportExportPresignedUrlTimeOfexpiration:
    Type: Number
    Description: Presigned URL Time of Expiration
    Default: 259200

  LambdaPnSupportExportSqsVisibilityTimeoutInSeconds:
    Type: Number
    Description: SQS queue visibility timeout Lambda export function for pn-support
    Default: 360
  
  LambdaPrepareBlockedAnalysisCronExpression:
    Type: String
    Description: Cron expression for the scheduled rule, e.g. cron(0 9 * * ? *)
    Default: 'cron(30 6 * * ? *)'

  LambdaPrepareBlockedAnalysisRepoZipUrl:
    Type: String
    Description: URL to download pn-troubleshooting repository ZIP (branch or tag)
    Default: ''

  TemplateBucketBaseUrl:
    Type: String
    Description: URL to load infrastructure model fragments from

  AlarmSNSTopicArn:
    Type: String

  DataAnalysisTopicName:
    Type: String
    Default: ''

Conditions:
  IsConfinfoAccount: !Not [ !Equals [!Ref PnCoreAwsAccountId, '' ] ]
  IsCoreAccount: !Equals [!Ref PnCoreAwsAccountId, '' ]
  IsDatalakeAccount: !Not [ !Equals [!Ref DataLakeAccountId, '' ] ]
  IsDatalakeAccountInCore: !And [ !Condition IsDatalakeAccount, !Condition IsCoreAccount ]
  HasCronExpression: !Not [ !Equals [!Ref DataMonitoringIndexCronExpression, '' ] ]
  HasDynamoDbExportTableNames: !Not [ !Equals [!Ref DynamoDbExportTableNames, '' ] ]
  HasCronExpressionLambdaPnSupportDynamoDBInCoreAccount: !And [!Not [!Equals [!Ref LambdaPnSupportExportDynamoDbCronExpression, '']], !Equals [!Ref PnCoreAwsAccountId, '']]
  HasCronExpressionLambdaPnSupportSqsInCoreAccount: !And [!Not [!Equals [!Ref LambdaPnSupportExportSqsCronExpression, '']], !Equals [!Ref PnCoreAwsAccountId, '']]

Resources:
  
  # s3 bucket with name BucketName
  DataMonitoringBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    UpdateReplacePolicy: Retain
    Properties:
      BucketName: !Sub pn-datamonitoring-${AWS::Region}-${AWS::AccountId}
      PublicAccessBlockConfiguration: 
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - BucketKeyEnabled: true
            ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256         
        
          
  DataMonitoringBucketPolicy:
    Condition: IsConfinfoAccount
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref DataMonitoringBucket
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Action:
              - "s3:Get*"
              - "s3:List*"              
            Resource: 
              - !Sub "arn:aws:s3:::${DataMonitoringBucket}"
              - !Sub "arn:aws:s3:::${DataMonitoringBucket}/*"   
            Principal:
              AWS: 
                - !Sub "arn:${AWS::Partition}:iam::${PnCoreAwsAccountId}:root"
            Condition:
              ArnLike:
                aws:PrincipalArn: 
                  - !Sub "arn:${AWS::Partition}:iam::${PnCoreAwsAccountId}:role/pn-data-monitoring-codebuild-role"
                  - !Sub "arn:${AWS::Partition}:iam::${PnCoreAwsAccountId}:role/pn-data-monitoring-ec2-service-role"
                  - !Sub "arn:${AWS::Partition}:iam::${PnCoreAwsAccountId}:instance-profile/pn-data-monitoring-ec2-instance-profile"
                  - !Sub "arn:${AWS::Partition}:sts::${PnCoreAwsAccountId}:assumed-role/pn-data-monitoring-ec2-service-role/*"
          - Effect: Allow
            Action:
              - "s3:PutObject"
            Resource: 
              - !Sub "arn:aws:s3:::${DataMonitoringBucket}/${ParquetBucketPath}/*"
            Principal:
              AWS: 
                - !Sub "arn:${AWS::Partition}:iam::${PnCoreAwsAccountId}:root"
            Condition:
              ArnLike:
                aws:PrincipalArn:
                  - !Sub "arn:${AWS::Partition}:iam::${PnCoreAwsAccountId}:role/pn-data-monitoring-codebuild-role"
                  - !Sub "arn:${AWS::Partition}:iam::${PnCoreAwsAccountId}:role/pn-data-monitoring-ec2-service-role"
                  - !Sub "arn:${AWS::Partition}:iam::${PnCoreAwsAccountId}:instance-profile/pn-data-monitoring-ec2-instance-profile"
                  - !Sub "arn:${AWS::Partition}:sts::${PnCoreAwsAccountId}:assumed-role/pn-data-monitoring-ec2-service-role/*"
          - Fn::If:
            - IsDatalakeAccount
            - Effect: Allow
              Action:
                - "s3:Get*"
                - "s3:List*"
              Resource:
                - !Sub "arn:aws:s3:::${DataMonitoringBucket}"
                - !Sub "arn:aws:s3:::${DataMonitoringBucket}/*"
              Principal:
                AWS:
                  - !Sub "arn:${AWS::Partition}:iam::${DataLakeAccountId}:root"
              Condition:
                ArnLike:
                  aws:PrincipalArn:
                    - !Sub "arn:${AWS::Partition}:iam::${DataLakeAccountId}:role/${DataLakeRoleName}"
            - !Ref AWS::NoValue

  DataMonitoringBucketPolicyCore:
    Condition: IsDatalakeAccountInCore
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref DataMonitoringBucket
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Action:
              - "s3:Get*"
              - "s3:List*"
            Resource:
              - !Sub "arn:aws:s3:::${DataMonitoringBucket}"
              - !Sub "arn:aws:s3:::${DataMonitoringBucket}/*"
            Principal:
              AWS:
                - !Sub "arn:${AWS::Partition}:iam::${DataLakeAccountId}:root"
            Condition:
              ArnLike:
                aws:PrincipalArn:
                  - !Sub "arn:${AWS::Partition}:iam::${DataLakeAccountId}:role/${DataLakeRoleName}"
  
  DataMonitoringCodebuildProject:
    Type: 'AWS::CodeBuild::Project'
    Properties:
      Name: pn-data-monitoring-codebuild
      ServiceRole: !GetAtt DataMonitoringCodeBuildServiceRole.Arn
      ConcurrentBuildLimit: 1
      TimeoutInMinutes: 360
      Source: 
        Type: NO_SOURCE
        BuildSpec: |
          version: 0.2
          phases:
            pre_build:
              commands:
                - ROOT_FOLDER=$(pwd) && echo ${ROOT_FOLDER}
                - git clone https://github.com/pagopa/pn-troubleshooting.git
                - git clone https://github.com/pagopa/pn-prototypes.git
                - ( cd ${ROOT_FOLDER}/pn-prototypes && git checkout $PROTOTYPE_VERSION )
                - ( cd ${ROOT_FOLDER}/pn-troubleshooting && git checkout $SCRIPTS_VERSION )
            build:
              commands:
                - ROOT_FOLDER=$(pwd) && echo ${ROOT_FOLDER}
                - |
                  cd pn-troubleshooting/bi_or_not_bi__ec_metadata
                  ./cicd_runner.sh \
                      --account-type $ACCOUNT_TYPE \
                      --env-type $ENV_TYPE \
                      --export-bucket-name $EXPORT_BUCKET_NAME \
                      --logs-bucket-name $LOGS_BUCKET_NAME \
                      --resource-root $ROOT_FOLDER/pn-prototypes \
                      --core-bucket-name $CORE_BUCKET \
                      --confinfo-bucket-name $CONFINFO_BUCKET \
                      --timestamp-utc "$DATE_EXECUTION"
      Artifacts:
        Type: NO_ARTIFACTS
      Environment:
        ComputeType: BUILD_GENERAL1_XLARGE
        Type: LINUX_CONTAINER
        Image: "aws/codebuild/standard:7.0-24.10.02"
        EnvironmentVariables:
          - Name: REGION
            Type: PLAINTEXT
            Value: !Ref AWS::Region
          - Name: EXPORT_BUCKET_NAME
            Type: PLAINTEXT
            Value: !Ref DataMonitoringBucket
          - Name: LOGS_BUCKET_NAME
            Type: PLAINTEXT
            Value: !Ref LogsBucketName
          - Name: ACCOUNT_TYPE
            Type: PLAINTEXT
            Value: !If [IsCoreAccount, "core", "confinfo"]
          - Name: ENV_TYPE
            Type: PLAINTEXT
            Value: !Ref EnvironmentType
          - Name: SCRIPTS_VERSION
            Type: PLAINTEXT
            Value: !Ref DataMonitoringScriptVersion
          - Name: PROTOTYPE_VERSION
            Type: PLAINTEXT
            Value: !Ref DataMonitoringPrototypeVersion
          - Name: CORE_BUCKET
            Type: PLAINTEXT
            Value: !If [IsCoreAccount, !Ref DataMonitoringBucket, !Sub "pn-datamonitoring-${AWS::Region}-${PnCoreAwsAccountId}"]
          - Name: CONFINFO_BUCKET
            Type: PLAINTEXT
            Value: !If [IsCoreAccount, !Ref ConfinfoDynamoDbExportBucketName, !Ref DataMonitoringBucket]
          - Name: DATE_EXECUTION
            Type: PLAINTEXT
            Value: ""

  DataMonitoringCodeBuildServiceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: pn-data-monitoring-codebuild-role
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "codebuild.amazonaws.com"
            Action:
              - "sts:AssumeRole"
          - Effect: Allow
            Principal:
              AWS:
                - !Sub "arn:${AWS::Partition}:iam::${AWS::AccountId}:root"
            Condition:
              ArnEquals:
                aws:PrincipalArn: !Sub arn:${AWS::Partition}:iam::${AWS::AccountId}:role/pn-data-monitoring-codebuild-role
            Action:
              - "sts:AssumeRole" 

  EC2DataMonitoringInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Condition: IsCoreAccount
    Properties:
      InstanceProfileName: pn-data-monitoring-ec2-instance-profile
      Path: "/"
      Roles:
        - !Ref EC2DataMonitoringServiceRole
  
  EC2DataMonitoringServiceRole:
    Type: AWS::IAM::Role
    Condition: IsCoreAccount
    Properties:
      RoleName: pn-data-monitoring-ec2-service-role
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "ec2.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore

  DataMonitoringEC2ManagedPolicy:
    Type: AWS::IAM::ManagedPolicy
    Condition: IsCoreAccount
    Properties:
      ManagedPolicyName: pn-data-monitoring-ec2-managed-policy
      Roles:
        - !Ref EC2DataMonitoringServiceRole
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          # lettura su tutto il bucket dei log e parquet
          - Effect: Allow
            Action:
              - "s3:Get*"
              - "s3:List*"
            Resource: 
              - !Sub "arn:aws:s3:::${LogsBucketName}"
              - !Sub "arn:aws:s3:::${LogsBucketName}/*"
              - !Sub "arn:aws:s3:::${DataMonitoringBucket}"
              - !Sub "arn:aws:s3:::${DataMonitoringBucket}/*"
          # Accesso in scrittura al bucket dump dell'account corrente nella cartella parquet
          - Effect: Allow
            Action:
              - "s3:PutObject"
              - "s3:DeleteObject"
            Resource: 
              - !Sub "arn:aws:s3:::${DataMonitoringBucket}/${ParquetBucketPath}/*"
              - !Sub "arn:aws:s3:::${DataMonitoringBucket}/${ReportsBucketPath}/*"
          # Accessi in lettura al bucket dump dell'account confinfo nella cartella parquet
          - Fn::If:
            - IsCoreAccount
            - Effect: Allow
              Action:
                - "s3:Get*"
                - "s3:List*"
              Resource: 
              - !Sub "arn:aws:s3:::${ConfinfoDynamoDbExportBucketName}"
              - !Sub "arn:aws:s3:::${ConfinfoDynamoDbExportBucketName}/${ParquetBucketPath}/*"
            - !Ref AWS::NoValue
          # Accesso in lettura al bucket di confinfo per protocollo S3A
          - Fn::If:
            - IsCoreAccount
            - Effect: Allow
              Action:
                - "s3:ListBucket"
              Resource:
                - !Sub "arn:aws:s3:::${ConfinfoDynamoDbExportBucketName}"
            - !Ref AWS::NoValue
          # Accessi in lettura ai log di pn-delivery
          - Fn::If:
            - IsCoreAccount
            - Effect: Allow
              Action:
                - "logs:StartQuery"
                - "logs:StopQuery"
                - "logs:GetQueryResults"
              Resource: 
              - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/ecs/pn-delivery:*"
            - !Ref AWS::NoValue
          # Allow the use of encryption key
          - Effect: Allow
            Action:
              - kms:Encrypt
              - kms:Decrypt
              - kms:ReEncrypt*
              - kms:GenerateDataKey*
              - kms:DescribeKey
            Resource: 
              - !Sub "${LogsBucketKmsKeyArn}"

  DataMonitoringCodebuildManagedPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      ManagedPolicyName: pn-data-monitoring-codebuild-managed-policy
      Roles:
        - !Ref DataMonitoringCodeBuildServiceRole
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Action:
              - logs:CreateLogGroup
              - logs:CreateLogStream
              - logs:PutLogEvents
            Resource: !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:*
          # lettura su tutto il bucket dei log e parquet
          - Effect: Allow
            Action:
              - "s3:Get*"
              - "s3:List*"
            Resource: 
              - !Sub "arn:aws:s3:::${LogsBucketName}"
              - !Sub "arn:aws:s3:::${LogsBucketName}/*"
              - !Sub "arn:aws:s3:::${DataMonitoringBucket}"
              - !Sub "arn:aws:s3:::${DataMonitoringBucket}/*"
          # Accesso in scrittura al bucket dump dell'account corrente nella cartella parquet
          - Effect: Allow
            Action:
              - "s3:PutObject"
              - "s3:DeleteObject"
            Resource: 
              - !Sub "arn:aws:s3:::${DataMonitoringBucket}/${ParquetBucketPath}/*"
              - !Sub "arn:aws:s3:::${DataMonitoringBucket}/${ReportsBucketPath}/*"
          # Accessi in lettura al bucket dump dell'account confinfo nella cartella parquet
          - Fn::If:
            - IsCoreAccount
            - Effect: Allow
              Action:
                - "s3:Get*"
                - "s3:List*"
              Resource: 
              - !Sub "arn:aws:s3:::${ConfinfoDynamoDbExportBucketName}/${ParquetBucketPath}/*"       
            - !Ref AWS::NoValue
          # Accesso in lettura al bucket di confinfo per protocollo S3A
          - Fn::If:
            - IsCoreAccount
            - Effect: Allow
              Action:
                - "s3:ListBucket"
              Resource:
                - !Sub "arn:aws:s3:::${ConfinfoDynamoDbExportBucketName}"
            - !Ref AWS::NoValue
          # Accessi in lettura ai log di pn-delivery
          - Fn::If:
            - IsCoreAccount
            - Effect: Allow
              Action:
                - "logs:StartQuery"
                - "logs:StopQuery"
                - "logs:GetQueryResults"
              Resource: 
              - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/ecs/pn-delivery:*"
            - !Ref AWS::NoValue
          # Allow the use of encryption key
          - Effect: Allow
            Action:
              - kms:Encrypt
              - kms:Decrypt
              - kms:ReEncrypt*
              - kms:GenerateDataKey*
              - kms:DescribeKey
            Resource: 
              - !Sub "${LogsBucketKmsKeyArn}"

  DataMonitoringScheduleRole:
    Condition: HasCronExpression
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Action: sts:AssumeRole
            Effect: Allow
            Principal:
              Service: events.amazonaws.com
        Version: "2012-10-17"
      Policies:
        - PolicyName: runCodeBuild
          PolicyDocument:
            Statement:
              - Sid: startProjectRun
                Action:
                  - "codebuild:*"
                Effect: Allow
                Resource: 
                  - !GetAtt DataMonitoringCodebuildProject.Arn

  IndexScheduledRule:
    Type: AWS::Events::Rule
    Condition: HasCronExpression
    Properties:
      Description: "pn-data-monitoring-codebuild-scheduled-rule"
      # every day at 9 am
      ScheduleExpression: !Ref DataMonitoringIndexCronExpression 
      State: "ENABLED"
      RoleArn: !GetAtt "DataMonitoringScheduleRole.Arn"
      Targets: 
        - Id: "DataMonitoringIndex"
          RoleArn: !GetAtt "DataMonitoringScheduleRole.Arn"
          # Input:
          #   !Sub |
          #     {
          #       "environmentVariablesOverride": {
          #         "cmd": ${Cmd}
          #       }
          #     }
          Arn: 
            Fn::GetAtt: 
              - "DataMonitoringCodebuildProject"
              - "Arn"
        
  DynamoDbExportFunction:
    Type: AWS::Lambda::Function
    Condition: HasDynamoDbExportTableNames
    Properties:
      Runtime: python3.12
      Handler: index.lambda_handler
      FunctionName: Lambda-DailyExport-DynamoDbTable
      Code:
        ZipFile: |
          import boto3
          import os
          import logging
          from datetime import datetime
          from datetime import timedelta
          
          def lambda_handler(event, context):
              dynamodb = boto3.client('dynamodb')
              dynamo_table_names = os.environ['DynamoDbExportTableNames'].split(',')
              s3_bucket = os.environ['S3Bucket']
              prefix = os.environ['Prefix']
              region = os.environ['Region']
              accountid = os.environ['AccountID']
              current_date = datetime.now().strftime('%Y%m%d')
              yesterday_date= (datetime.strptime(current_date, '%Y%m%d') - timedelta(days=1)).strftime('%Y%m%d')

              for dynamo_table_name in dynamo_table_names:
                  dynamo_table_arn = f"arn:aws:dynamodb:{region}:{accountid}:table/{dynamo_table_name}"
                  s3_full_prefix = dynamo_table_name + "/" + prefix + "/" + yesterday_date
                  now = datetime.now()
                  to_time = now.replace(hour=0, minute=0, second=0, microsecond=0)
                  from_time = to_time - timedelta(days=1)
                  try:
                      dynamodb.export_table_to_point_in_time(
                                  TableArn=dynamo_table_arn,
                                  S3Bucket=s3_bucket,
                                  S3Prefix=s3_full_prefix,
                                  ExportFormat='DYNAMODB_JSON',
                                  ExportType='INCREMENTAL_EXPORT',
                                  IncrementalExportSpecification={
                                      'ExportFromTime': from_time,
                                      'ExportToTime': to_time,
                                      'ExportViewType': 'NEW_IMAGE'
                                  }
                              )
                      print(f"Tabella '{dynamo_table_name}' export start.")
                  except Exception as e:
                      print(f"Error during l'export of table '{dynamo_table_name}': {e}")

      Environment:
        Variables:
          DynamoDbExportTableNames:
            Ref: DynamoDbExportTableNames
          S3Bucket:
            Ref: DataMonitoringBucket
          Prefix:
            Ref: DynamoDbExportPrefix
          Region: !Ref AWS::Region
          AccountID: !Ref AWS::AccountId


      Role:
        Fn::GetAtt:
          - ExportRole
          - Arn

  ExportRole:
    Type: AWS::IAM::Role
    Condition: HasDynamoDbExportTableNames
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: ExportPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - dynamodb:Scan
                  - dynamodb:ExportTableToPointInTime
                Resource: arn:aws:dynamodb:*:*:*
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource:
                  - !Sub arn:aws:s3:::${DataMonitoringBucket}/*
              - Effect: Allow
                Action:
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource: arn:aws:logs:*:*:*
              - Effect: Allow
                Action:
                  - "kms:Decrypt"
                Resource: "*"

  Schedule:
    Type: AWS::Events::Rule
    Condition: HasDynamoDbExportTableNames
    Properties:
      ScheduleExpression: cron(0 1 * * ? *)
      State: ENABLED
      Targets:
        - Arn: !GetAtt DynamoDbExportFunction.Arn
          Id: DynamoDbExportFunction

  PermissionForEventsToInvokeLambda:
    Type: AWS::Lambda::Permission
    Condition: HasDynamoDbExportTableNames
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref DynamoDbExportFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt Schedule.Arn
      SourceAccount: !Ref AWS::AccountId

  ExportFunctionLogroup:
    Type: AWS::Logs::LogGroup
    Condition: HasDynamoDbExportTableNames
    DeletionPolicy: Delete
    UpdateReplacePolicy : Delete
    Properties:
      LogGroupName: !Sub "/aws/lambda/${DynamoDbExportFunction}"
      RetentionInDays: 3
  

  # Send codebuild events to Frankfurt; Needed by celonis :(
  SendCodeBuildEventsToFrankfurt: 
    Type: AWS::Events::Rule
    Properties: 
      Description: "Routes datamonitoring ends to eu-central-1"
      State: "ENABLED"
      EventPattern:
        source:  [ "aws.codebuild" ]
        detail-type: [ "CodeBuild Build State Change" ]
        detail: 
          project-name: [ "pn-data-monitoring-codebuild" ]
      Targets: 
        - Arn: !Sub "arn:${AWS::Partition}:events:eu-central-1:${AWS::AccountId}:event-bus/default"
          Id: "CrossRegionDestinationBus"
          RoleArn: !GetAtt SendCodeBuildEventsToFrankfurtRole.Arn
  
  SendCodeBuildEventsToFrankfurtRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
        - Effect: Allow
          Principal:
            Service: events.amazonaws.com
          Action: sts:AssumeRole
      Path: /
      Policies:
      - PolicyName: PutEventsDestinationBus
        PolicyDocument:
          Version: 2012-10-17
          Statement:
          - Effect: Allow
            Action:
            - events:PutEvents
            Resource:
            - !Sub "arn:${AWS::Partition}:events:eu-central-1:${AWS::AccountId}:event-bus/default"

  LambdaPnSupportExport:
    Condition: IsCoreAccount
    Type: AWS::Lambda::Function
    Properties:
      Runtime: python3.12
      Timeout: 600
      MemorySize: 384
      Handler: index.lambda_handler
      FunctionName: !Sub pn-support-export-lambda-${EnvironmentType}
      Code:
        ZipFile: |
          import os
          import boto3
          import csv
          import io
          from datetime import datetime
          from botocore.config import Config

          dynamodb = boto3.resource('dynamodb')
          s3 = boto3.client(
              's3',
              region_name=os.environ['Region'],
              config=Config(s3={'addressing_style': 'virtual'})
          )
          sns = boto3.client('sns')
          sqs = boto3.client('sqs', region_name=os.environ['Region'])

          def lambda_handler(event, context):
              dump_type = event.get('DumpType')
              if not dump_type:
                  raise ValueError("Error: missing 'DumpType' parameter in event. Must be 'DYNAMODB' or 'SQS'.")
              dump_type = dump_type.upper()

              s3_bucket = os.environ['S3Bucket']
              prefix = os.environ['Prefix']
              region = os.environ['Region']
              sns_topic_arn = os.environ['SnsTopicArn']
              presignedurltime = int(os.environ['PresignedUrlTime'])
              timestamp = datetime.now().strftime('%Y%m%d_%H%M')

              presigned_url = None
              s3_key = None
              subject = None
              message = None

              if dump_type == 'DYNAMODB':
                  table_name = os.environ['DynamoDbTableLambdaDump']
                  delete_items = os.environ.get('DeleteItems', 'false').lower() == 'true'
                  category_filter = 'RENDICONTAZIONE_SCARTATA'
                  table = dynamodb.Table(table_name)

                  print(f"Running DYNAMODB dump from table {table_name}")
                  all_items = []
                  exclusive_start_key = None

                  while True:
                      scan_params = {
                          'FilterExpression': 'category = :val',
                          'ExpressionAttributeValues': {':val': category_filter}
                      }
                      if exclusive_start_key:
                          scan_params['ExclusiveStartKey'] = exclusive_start_key

                      response = table.scan(**scan_params)
                      all_items.extend(response.get('Items', []))
                      exclusive_start_key = response.get('LastEvaluatedKey')
                      if not exclusive_start_key:
                          break

                  if all_items:
                      csv_buffer = io.StringIO()
                      headers = all_items[0].keys()
                      writer = csv.DictWriter(csv_buffer, fieldnames=headers)
                      writer.writeheader()
                      for item in all_items:
                          writer.writerow(item)

                      s3_key = f"{prefix}/CSV_{table_name}_{timestamp}.csv"
                      s3.put_object(Bucket=s3_bucket, Key=s3_key, Body=csv_buffer.getvalue())

                      presigned_url = s3.generate_presigned_url(
                          ClientMethod='get_object',
                          Params={'Bucket': s3_bucket, 'Key': s3_key},
                          ExpiresIn=presignedurltime
                      )

                  if delete_items:
                      for item in all_items:
                          try:
                              table.delete_item(Key={'requestId': item['requestId'], 'created': item['created']})
                          except Exception as e:
                              print(f"Error deleting item {item}: {e}")

                  subject = f"CSV RENDICONTAZIONE SCARTATA {timestamp}"
                  message = f"DYNAMODB dump completed.\n\nCSV file: {presigned_url if presigned_url else 'No data found.'}"

              elif dump_type == 'SQS':
                  queue_name = os.environ['SqsDumpQueueName']
                  visibility_timeout = int(os.environ.get('SqsDumpVisibilityTimeout', '5'))

                  try:
                      queue_url = sqs.get_queue_url(QueueName=queue_name)['QueueUrl']
                  except Exception as e:
                      print(f"Error getting URL for queue {queue_name}: {e}")
                      queue_url = None

                  all_messages = []
                  if queue_url:
                      print(f"Reading messages from queue: {queue_name}")
                      while True:
                          response = sqs.receive_message(
                              QueueUrl=queue_url,
                              MaxNumberOfMessages=10,
                              VisibilityTimeout=visibility_timeout,
                              WaitTimeSeconds=1
                          )
                          msgs = response.get('Messages', [])
                          if not msgs:
                              break

                          for m in msgs:
                              all_messages.append({
                                  'MessageId': m['MessageId'],
                                  'Body': m['Body']
                              })

                      print(f"Found {len(all_messages)} messages.")

                      if all_messages:
                          csv_buffer = io.StringIO()
                          writer = csv.DictWriter(csv_buffer, fieldnames=all_messages[0].keys())
                          writer.writeheader()
                          for msg in all_messages:
                              writer.writerow(msg)

                          safe_queue_name = queue_name.replace('/', '_')
                          s3_key = f"{prefix}/CSV_{safe_queue_name}_{timestamp}.csv"
                          s3.put_object(Bucket=s3_bucket, Key=s3_key, Body=csv_buffer.getvalue())

                          presigned_url = s3.generate_presigned_url(
                              ClientMethod='get_object',
                              Params={'Bucket': s3_bucket, 'Key': s3_key},
                              ExpiresIn=presignedurltime
                          )

                  subject = f"CSV {queue_name} {timestamp}"
                  message = (
                      f"SQS dump completed.\n\n"
                      f"Messages found: {len(all_messages)}\n"
                      f"CSV file: {presigned_url if presigned_url else 'No messages available.'}"
                  )

              else:
                  raise ValueError("Error: invalid 'DumpType' parameter. Must be 'DYNAMODB' or 'SQS'.")

              sns.publish(
                  TopicArn=sns_topic_arn,
                  Subject=subject,
                  Message=message
              )

              return {
                  'statusCode': 200,
                  'body': message
              }

      Environment:
        Variables:
          DynamoDbTableLambdaDump: pn-PaperRequestError
          S3Bucket: !Ref DataMonitoringBucket
          Prefix: LambdaPnSupport-Export
          DeleteItems: !Ref LambdaPnSupportExportDelete
          SnsTopicArn: !Ref DataAnalysisTopic
          Region: !Ref AWS::Region
          AccountID: !Ref AWS::AccountId
          PresignedUrlTime: !Ref LambdaPnSupportExportPresignedUrlTimeOfexpiration
          SqsDumpQueueName: pn-external_channel_to_paper_channel_dry_run-DLQ
          SqsDumpVisibilityTimeoutInSeconds: !Ref LambdaPnSupportExportSqsVisibilityTimeoutInSeconds


      Role:
        Fn::GetAtt:
          - LambdaPnSupportExportRole
          - Arn

  LambdaPnSupportExportAlarms:
    Condition: IsCoreAccount
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: !Sub "${TemplateBucketBaseUrl}/fragments/lambda-alarms.yaml"
      Parameters:
        FunctionName: !Ref LambdaPnSupportExport
        AlarmSNSTopicArn: !Ref AlarmSNSTopicArn
        FilterPattern: "ERROR"

  LambdaPnSupportExportRole:
    Condition: IsCoreAccount
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: LambdaPnSupportExportPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - dynamodb:Scan
                  - dynamodb:DeleteItem
                Resource: !Sub arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/pn-PaperRequestError
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                Resource:
                  - !Sub arn:aws:s3:::${DataMonitoringBucket}/LambdaPnSupport-Export/*
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: arn:aws:logs:*:*:*
              - Effect: Allow
                Action:
                  - kms:Decrypt
                Resource: "*"
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: !Ref DataAnalysisTopic
              - Effect: Allow
                Action:
                  - sqs:GetQueueUrl
                  - sqs:ReceiveMessage
                Resource: !Sub  arn:aws:sqs:${AWS::Region}:${AWS::AccountId}:pn-external_channel_to_paper_channel_dry_run-DLQ

  DataAnalysisTopic:
    Condition: IsCoreAccount
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Ref DataAnalysisTopicName

  DataAnalysisTopicSubscription:
    Condition: IsCoreAccount
    Type: AWS::SNS::Subscription
    Properties:
      Protocol: email
      Endpoint: !Ref DataAnalysisSlackEmail
      TopicArn: !Ref DataAnalysisTopic

  LambdaSNSTopicPolicy:
    Condition: IsCoreAccount
    Type: AWS::SNS::TopicPolicy
    Properties:
      Topics:
        - !Ref DataAnalysisTopic
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
              - lambda.amazonaws.com
            Action: sns:Publish
            Resource: !Ref DataAnalysisTopic

  ScheduleExportDynamoDb:
    Condition: HasCronExpressionLambdaPnSupportDynamoDBInCoreAccount
    Type: AWS::Events::Rule
    Properties:
      ScheduleExpression: !Ref LambdaPnSupportExportDynamoDbCronExpression
      State: ENABLED
      Targets:
        - Arn: !GetAtt LambdaPnSupportExport.Arn
          Id: LambdaPnSupportExportDynamoDb
          Input: |
            {
              "DumpType": "DYNAMODB"
            }
    
  ScheduleExportSqs:
    Condition: HasCronExpressionLambdaPnSupportSqsInCoreAccount
    Type: AWS::Events::Rule
    Properties:
      ScheduleExpression: !Ref LambdaPnSupportExportSqsCronExpression
      State: ENABLED
      Targets:
        - Arn: !GetAtt LambdaPnSupportExport.Arn
          Id: LambdaPnSupportExportSqs
          Input: |
            {
              "DumpType": "SQS"
            }

  PermissionForEventsToInvokeLambdaPnSupportExportDB:
    Condition: HasCronExpressionLambdaPnSupportDynamoDBInCoreAccount
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref LambdaPnSupportExport
      Principal: events.amazonaws.com
      SourceArn: !GetAtt ScheduleExportDynamoDb.Arn
      SourceAccount: !Ref AWS::AccountId

  PermissionForEventsToInvokeLambdaPnSupportExportSQS:
    Condition: HasCronExpressionLambdaPnSupportSqsInCoreAccount
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref LambdaPnSupportExport
      Principal: events.amazonaws.com
      SourceArn: !GetAtt ScheduleExportSqs.Arn
      SourceAccount: !Ref AWS::AccountId

  LambdaPnSupportExportLogroup:
    Condition: IsCoreAccount
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Delete
    UpdateReplacePolicy : Delete
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaPnSupportExport}"
      RetentionInDays: 3

  # Lambda for Blocked Analysis
  LambdaPrepareBlockedAnalysis:
    Condition: IsCoreAccount
    Type: AWS::Lambda::Function
    Properties:
      Runtime: python3.14
      Timeout: 900
      MemorySize: 512
      Handler: index.lambda_handler
      FunctionName: !Sub pn-prepare-blocked-analysis-lambda-${EnvironmentType}
      Code:
        ZipFile: |
          import os
          import boto3
          import subprocess
          import sys
          import shutil
          import urllib.request
          import zipfile
          from pathlib import Path

          def lambda_handler(event, context):
              print("Starting PrepareBlockedAnalysis Lambda")
              
              # Get environment variables
              region = os.environ['Region']
              database = os.environ.get('AthenaDatabase', 'cdc_analytics_database')
              table = os.environ.get('AthenaTable', 'pn_timelines_json_view')
              workgroup = os.environ.get('AthenaWorkgroup', 'primary')
              s3_result_bucket = os.environ['S3ResultBucket']
              athena_results_bucket = os.environ['AthenaResultsBucket']
              repo_zip_url = os.environ['RepoZipUrl']
              
              # Download pn-troubleshooting repository as ZIP
              zip_path = "/tmp/pn-troubleshooting.zip"
              repo_path = "/tmp/pn-troubleshooting"
              
              # Clean up if exists
              if os.path.exists(repo_path):
                  shutil.rmtree(repo_path)
              if os.path.exists(zip_path):
                  os.remove(zip_path)
              
              print(f"Downloading pn-troubleshooting repository from {repo_zip_url}...")
              try:
                  urllib.request.urlretrieve(repo_zip_url, zip_path)
                  print("Repository downloaded successfully")
                  
                  # Extract ZIP
                  print("Extracting ZIP...")
                  with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                      zip_ref.extractall("/tmp")
                  
                  # Find extracted folder (works with any branch name)
                  import glob
                  extracted_folders = glob.glob("/tmp/pn-troubleshooting-*")
                  if extracted_folders:
                      extracted_folder = extracted_folders[0]
                      print(f"Found extracted folder: {extracted_folder}")
                      if os.path.exists(repo_path):
                          shutil.rmtree(repo_path)
                      os.rename(extracted_folder, repo_path)
                  else:
                      raise FileNotFoundError("Extracted folder not found")
                  
                  print("Repository extracted successfully")
              except Exception as e:
                  print(f"Error downloading/extracting repository: {e}")
                  raise
              
              # Change to script directory
              script_dir = os.path.join(repo_path, "prepare_blocked_analisys")
              script_path = os.path.join(script_dir, "prepare_blocked_analisys.py")
              
              if not os.path.exists(script_path):
                  print(f"Script not found at {script_path}")
                  raise FileNotFoundError(f"Script not found at {script_path}")
              
              print(f"Found script at {script_path}")
              
              # Execute the script
              print("Executing prepare_blocked_analisys.py...")
              
              cmd = [
                  sys.executable,
                  script_path,
                  "--database", database,
                  "--table", table,
                  "--workgroup", workgroup,
                  "--output-location", f"s3://{athena_results_bucket}/",
                  "--s3-result-bucket", s3_result_bucket,
                  "--timeout", "840"  # 60 secondi meno del timeout Lambda per salvare i progressi
              ]
              
              print(f"Running command: {' '.join(cmd)}")
              
              try:
                  # Run the script
                  result = subprocess.run(
                      cmd,
                      check=True,
                      capture_output=True,
                      text=True,
                      cwd=script_dir,
                      env={**os.environ, 'AWS_DEFAULT_REGION': region}
                  )
                  print("Script output:")
                  print(result.stdout)
                  if result.stderr:
                      print("Script stderr:")
                      print(result.stderr)
                  
                  # Copy result files to S3
                  result_dir = os.path.join(script_dir, "result")
                  if os.path.exists(result_dir):
                      print(f"Copying result files from {result_dir} to S3...")
                      s3_client = boto3.client('s3', region_name=region)
                      bucket_name = s3_result_bucket.replace('s3://', '').split('/')[0]
                      s3_prefix = '/'.join(s3_result_bucket.replace('s3://', '').split('/')[1:])
                      
                      uploaded_files = []
                      for filename in os.listdir(result_dir):
                          file_path = os.path.join(result_dir, filename)
                          if os.path.isfile(file_path):
                              s3_key = f"{s3_prefix}/{filename}".lstrip('/')
                              print(f"Uploading {filename} to s3://{bucket_name}/{s3_key}")
                              s3_client.upload_file(file_path, bucket_name, s3_key)
                              uploaded_files.append(filename)
                      
                      print(f"Successfully uploaded {len(uploaded_files)} files to S3: {uploaded_files}")
                  else:
                      print(f"Result directory not found at {result_dir}")
                  
                  return {
                      'statusCode': 200,
                      'body': 'PrepareBlockedAnalysis completed successfully'
                  }
              except subprocess.CalledProcessError as e:
                  print(f"Script execution failed with return code {e.returncode}")
                  print(f"stdout: {e.stdout}")
                  print(f"stderr: {e.stderr}")
                  
                  # Return code 2 means timeout, which is acceptable
                  if e.returncode == 2:
                      print("Script reached timeout but saved progress")
                      
                      # Try to upload any result files even after timeout
                      result_dir = os.path.join(script_dir, "result")
                      if os.path.exists(result_dir):
                          print(f"Attempting to copy result files after timeout...")
                          try:
                              s3_client = boto3.client('s3', region_name=region)
                              bucket_name = s3_result_bucket.replace('s3://', '').split('/')[0]
                              s3_prefix = '/'.join(s3_result_bucket.replace('s3://', '').split('/')[1:])
                              
                              uploaded_files = []
                              for filename in os.listdir(result_dir):
                                  file_path = os.path.join(result_dir, filename)
                                  if os.path.isfile(file_path):
                                      s3_key = f"{s3_prefix}/{filename}".lstrip('/')
                                      s3_client.upload_file(file_path, bucket_name, s3_key)
                                      uploaded_files.append(filename)
                              
                              print(f"Uploaded {len(uploaded_files)} files after timeout: {uploaded_files}")
                          except Exception as upload_error:
                              print(f"Error uploading files after timeout: {upload_error}")
                      
                      return {
                          'statusCode': 200,
                          'body': 'PrepareBlockedAnalysis reached timeout but progress was saved'
                      }
                  raise

      Environment:
        Variables:
          Region: !Ref AWS::Region
          AthenaDatabase: cdc_analytics_database
          AthenaTable: pn_timelines_json_view
          AthenaWorkgroup: primary
          S3ResultBucket: !Sub s3://${DataMonitoringBucket}/prepare-blocked-analysis/
          AthenaResultsBucket: !Ref AthenaResultsBucketName
          RepoZipUrl: !Ref LambdaPrepareBlockedAnalysisRepoZipUrl
      Role: !GetAtt LambdaPrepareBlockedAnalysisRole.Arn

  LambdaPrepareBlockedAnalysisRole:
    Condition: IsCoreAccount
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub pn-prepare-blocked-analysis-lambda-role-${EnvironmentType}
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: LambdaPrepareBlockedAnalysisPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              # S3 write access for results
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:ListBucket
                  - s3:GetBucketLocation
                  - s3:GetBucketVersioning
                Resource:
                  - !Sub arn:aws:s3:::${DataMonitoringBucket}
                  - !Sub arn:aws:s3:::${DataMonitoringBucket}/*
                  - !Sub arn:aws:s3:::${AthenaResultsBucketName}
                  - !Sub arn:aws:s3:::${AthenaResultsBucketName}/*
              # S3 read-only access for logs bucket (Athena data source)
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:GetBucketLocation
                Resource:
                  - !Sub arn:aws:s3:::${LogsBucketName}
                  - !Sub arn:aws:s3:::${LogsBucketName}/*
              # S3 additional permissions for Athena bucket verification
              - Effect: Allow
                Action:
                  - s3:ListAllMyBuckets
                  - s3:GetBucketLocation
                Resource: "*"
              # Athena permissions
              - Effect: Allow
                Action:
                  - athena:StartQueryExecution
                  - athena:GetQueryExecution
                  - athena:GetQueryResults
                  - athena:GetWorkGroup
                Resource:
                  - !Sub arn:aws:athena:${AWS::Region}:${AWS::AccountId}:workgroup/*
              # Glue permissions for Athena
              - Effect: Allow
                Action:
                  - glue:GetDatabase
                  - glue:GetTable
                  - glue:GetPartitions
                Resource:
                  - !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:catalog
                  - !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:database/*
                  - !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:table/*/*
              # DynamoDB access for pn-PaperRequestError
              - Effect: Allow
                Action:
                  - dynamodb:Query
                  - dynamodb:Scan
                Resource:
                  - !Sub arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/pn-PaperRequestError
              # KMS permissions for logs bucket encryption
              - Effect: Allow
                Action:
                  - kms:Decrypt
                  - kms:DescribeKey
                Resource:
                  - !Sub "${LogsBucketKmsKeyArn}"

  LambdaPrepareBlockedAnalysisLogGroup:
    Condition: IsCoreAccount
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Delete
    UpdateReplacePolicy: Delete
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaPrepareBlockedAnalysis}"
      RetentionInDays: 7

  LambdaPrepareBlockedAnalysisAlarms:
    Condition: IsCoreAccount
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: !Sub "${TemplateBucketBaseUrl}/fragments/lambda-alarms.yaml"
      Parameters:
        FunctionName: !Ref LambdaPrepareBlockedAnalysis
        AlarmSNSTopicArn: !Ref AlarmSNSTopicArn
        FilterPattern: "ERROR"

  # EventBridge rule to trigger PrepareBlockedAnalysis Lambda
  LambdaPrepareBlockedAnalysisScheduleRule:
    Condition: IsCoreAccount
    Type: AWS::Events::Rule
    Properties:
      Description: "Scheduled rule to trigger PrepareBlockedAnalysis Lambda UTC"
      ScheduleExpression: !Ref LambdaPrepareBlockedAnalysisCronExpression
      State: ENABLED
      Targets:
        - Arn: !GetAtt LambdaPrepareBlockedAnalysis.Arn
          Id: LambdaPrepareBlockedAnalysisTarget

  LambdaPrepareBlockedAnalysisPermission:
    Condition: IsCoreAccount
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref LambdaPrepareBlockedAnalysis
      Principal: events.amazonaws.com
      SourceArn: !GetAtt LambdaPrepareBlockedAnalysisScheduleRule.Arn
      SourceAccount: !Ref AWS::AccountId

Outputs:
  DataMonitoringBucketName: 
    Value: !Ref DataMonitoringBucket
    Description: Name of the bucket created for the data monitoring
